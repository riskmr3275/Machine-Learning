{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae373548-62e3-46be-b97b-330bfa1e9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF to extract text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58689296-7db6-4bb4-a51a-e1d52de0f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Text from PDF\n",
    "print(\"Hello World\")\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b956796b-149f-484b-98ae-ebf4382d1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF\n",
    "pdf_text = extract_text_from_pdf(\"1169.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b087314-1546-4af7-b6c5-a5354911a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_split_text(text, chunk_size=1000):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive spaces\n",
    "    text_chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return text_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047356fd-9501-4f5c-bec7-b8c7417a61c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def search_relevant_chunk(question, text_chunks):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    corpus = text_chunks + [question]\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1])\n",
    "    relevant_chunk_idx = cosine_sim.argmax()  # Get the index of the most similar chunk\n",
    "    return text_chunks[relevant_chunk_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a450a174-24e7-43de-ac9b-a8833023030b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer_from_chunk(chunk, question):\n",
    "    # Simple approach: extract the sentence(s) that contain keywords\n",
    "    sentences = chunk.split(\".\")\n",
    "    relevant_sentences = [s for s in sentences if question.lower() in s.lower()]\n",
    "    return \" \".join(relevant_sentences) if relevant_sentences else \"Answer not found.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28895ee6-647d-41d9-ab63-fc749f499aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_pipeline(pdf_path, question):\n",
    "    # Step 1: Extract text from PDF\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Step 2: Preprocess and split the text into chunks\n",
    "    text_chunks = clean_and_split_text(pdf_text)\n",
    "    \n",
    "    # Step 3: Search for the most relevant chunk\n",
    "    relevant_chunk = search_relevant_chunk(question, text_chunks)\n",
    "    \n",
    "    # Step 4: Generate the answer from the relevant chunk\n",
    "    answer = extract_answer_from_chunk(relevant_chunk, question)\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab77540-661b-449e-b340-dc7162ee5b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"1169.pdf\" \n",
    "question = \"Who is the main character in the novel?\"\n",
    "print(\"Hello WOrld\")\n",
    "answer = qa_pipeline(pdf_path, question)\n",
    "print(answer)\n",
    "print(\"Hello WOrld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cad42-db6e-460c-9184-641f5a7d1a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a6a9f-82be-4500-a47e-2529aa1cb6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2ef17-11f0-4270-9198-d0c7b76ebca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994e85d-d89a-42de-b126-d5cd775c46e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bab9b-a9fe-488e-b92b-0a70330a4466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba3661-a2b7-45bc-ab48-e05fb1fffa10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e61035-d4f4-4053-b94c-6e9b503b8408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7a3bd-1bc6-441f-806d-f792ac743e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155154b8-3aa9-4d4b-91a9-82e62f3ad2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
